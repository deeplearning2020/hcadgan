{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38626f13-33c3-4f40-af22-a7609e2188ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import scipy.io as sio\n",
    "import os\n",
    "import requests\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import kornia.morphology as morph\n",
    "from einops import rearrange\n",
    "\n",
    "class MultiScaleBlock(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(MultiScaleBlock, self).__init__()\n",
    "        self.branch1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, in_channels // 4, 1),\n",
    "            nn.BatchNorm2d(in_channels // 4),\n",
    "            nn.GELU()\n",
    "        )\n",
    "        self.branch2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, in_channels // 4, 3, padding=1, dilation=1),\n",
    "            nn.BatchNorm2d(in_channels // 4),\n",
    "            nn.GELU()\n",
    "        )\n",
    "        self.branch3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, in_channels // 4, 3, padding=2, dilation=2),\n",
    "            nn.BatchNorm2d(in_channels // 4),\n",
    "            nn.GELU()\n",
    "        )\n",
    "        self.branch4 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, in_channels // 4, 3, padding=4, dilation=4),\n",
    "            nn.BatchNorm2d(in_channels // 4),\n",
    "            nn.GELU()\n",
    "        )\n",
    "        self.fusion = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, in_channels, 1),\n",
    "            nn.BatchNorm2d(in_channels),\n",
    "            nn.GELU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b1 = self.branch1(x)\n",
    "        b2 = self.branch2(x)\n",
    "        b3 = self.branch3(x)\n",
    "        b4 = self.branch4(x)\n",
    "        out = torch.cat([b1, b2, b3, b4], dim=1)\n",
    "        return self.fusion(out)\n",
    "\n",
    "class EnhancedMorphologicalAttention(nn.Module):\n",
    "    def __init__(self, in_channels, num_heads=8):\n",
    "        super(EnhancedMorphologicalAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = in_channels // num_heads\n",
    "        self.scale = (self.head_dim) ** -0.5\n",
    "        self.temperature = nn.Parameter(torch.ones(num_heads, 1, 1))\n",
    "        self.qkv = nn.Conv2d(in_channels, in_channels * 3, 1)\n",
    "        self.proj = nn.Conv2d(in_channels, in_channels, 1)\n",
    "        self.spatial_gate = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, in_channels, 3, padding=1, groups=in_channels),\n",
    "            nn.BatchNorm2d(in_channels),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(in_channels, in_channels, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.channel_mixer = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Conv2d(in_channels, in_channels // 4, 1),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(in_channels // 4, in_channels, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.morph_process = nn.Sequential(\n",
    "            nn.Conv2d(in_channels * 2, in_channels, 1),\n",
    "            nn.BatchNorm2d(in_channels),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(in_channels, in_channels, 3, padding=1, groups=in_channels),\n",
    "            nn.Conv2d(in_channels, in_channels, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        self.norm1 = nn.BatchNorm2d(in_channels)\n",
    "        self.norm2 = nn.BatchNorm2d(in_channels)\n",
    "        self.drop_path = nn.Dropout(0.6)\n",
    "        self.gamma = nn.Parameter(torch.zeros(1))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        qkv = self.qkv(x)\n",
    "        qkv = qkv.reshape(B, 3, self.num_heads, self.head_dim, H, W)\n",
    "        qkv = qkv.permute(0, 1, 2, 3, 4, 5).contiguous()\n",
    "        qkv = qkv.view(B, 3, self.num_heads, self.head_dim, -1)\n",
    "        q, k, v = qkv[:, 0], qkv[:, 1], qkv[:, 2]\n",
    "        q = q * self.scale\n",
    "        attn = torch.einsum('bhnm,bhkm->bhnk', q, k)\n",
    "        attn = attn * self.temperature\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        x = torch.einsum('bhnk,bhkm->bhnm', attn, v)\n",
    "        x = x.view(B, self.num_heads * self.head_dim, H, W)\n",
    "        x = self.proj(x)\n",
    "        kernel = torch.ones(3, 3).to(x.device)\n",
    "        dilated = morph.dilation(x, kernel)\n",
    "        eroded = morph.erosion(x, kernel)\n",
    "        morph_features = torch.cat([dilated, eroded], dim=1)\n",
    "        morph_gate = self.morph_process(morph_features)\n",
    "        spatial_attn = self.spatial_gate(x)\n",
    "        channel_attn = self.channel_mixer(x)\n",
    "        x = x * morph_gate * spatial_attn * channel_attn\n",
    "        x = shortcut + self.drop_path(x)\n",
    "        x = x + self.drop_path(self.gamma * self.norm2(x))\n",
    "        return x\n",
    "\n",
    "class EnhancedResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(EnhancedResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, in_channels, 3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(in_channels)\n",
    "        self.conv2 = nn.Conv2d(in_channels, in_channels, 3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(in_channels)\n",
    "        self.attention = EnhancedMorphologicalAttention(in_channels)\n",
    "        self.multiscale = MultiScaleBlock(in_channels)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.gelu = nn.GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.gelu(self.bn1(self.conv1(x)))\n",
    "        out = self.dropout(out)\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out = self.attention(out)\n",
    "        out = self.multiscale(out)\n",
    "        out += residual\n",
    "        return self.gelu(out)\n",
    "\n",
    "class RobustEnergyFunction(nn.Module):\n",
    "    def __init__(self, input_channels, num_classes):\n",
    "        super(RobustEnergyFunction, self).__init__()\n",
    "        self.initial_conv = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, 128, 3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.3)\n",
    "        )\n",
    "        self.encoder = nn.ModuleList([\n",
    "            EnhancedResidualBlock(128),\n",
    "            nn.Sequential(\n",
    "                nn.Conv2d(128, 256, 3, padding=1),\n",
    "                nn.BatchNorm2d(256),\n",
    "                nn.GELU(),\n",
    "                nn.MaxPool2d(2)\n",
    "            ),\n",
    "            EnhancedResidualBlock(256),\n",
    "            EnhancedResidualBlock(256)\n",
    "        ])\n",
    "        self.energy_head = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "        self.auxiliary_head = nn.Sequential(\n",
    "            nn.Conv2d(256, 128, 1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.GELU(),\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.initial_conv(x)\n",
    "        for layer in self.encoder:\n",
    "            x = layer(x)\n",
    "        energy = self.energy_head(x)\n",
    "        aux = self.auxiliary_head(x)\n",
    "        if self.training:\n",
    "            return energy, aux\n",
    "        return energy\n",
    "\n",
    "class RobustHyperspectralEBM(nn.Module):\n",
    "    def __init__(self, input_channels, num_classes, device):\n",
    "        super(RobustHyperspectralEBM, self).__init__()\n",
    "        self.energy_net = RobustEnergyFunction(input_channels, num_classes)\n",
    "        self.device = device\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.energy_net(x)\n",
    "\n",
    "    def sample(self, x, num_steps=15, step_size=0.05):\n",
    "        x_k = x.clone().requires_grad_(True)\n",
    "        for _ in range(num_steps):\n",
    "            energy = self.energy_net(x_k)[0] if self.training else self.energy_net(x_k)\n",
    "            grad = torch.autograd.grad(energy.sum(), x_k)[0]\n",
    "            x_k.data += step_size * grad\n",
    "            x_k.data += torch.randn_like(x_k) * np.sqrt(step_size * 2)\n",
    "        return x_k.detach()\n",
    "\n",
    "    def compute_loss(self, x_pos, x_neg, y):\n",
    "        pos_energy, aux = self.energy_net(x_pos)\n",
    "        neg_energy = self.energy_net(x_neg)[0] if self.training else self.energy_net(x_neg)\n",
    "        pos_loss = F.cross_entropy(pos_energy, y, label_smoothing=0.1)\n",
    "        aux_loss = F.cross_entropy(aux, y, label_smoothing=0.1)\n",
    "        reg_loss = 0.5 * (pos_energy.pow(2).mean() + neg_energy.pow(2).mean())\n",
    "        contrastive_loss = torch.mean(torch.relu(1.0 - (pos_energy - neg_energy)))\n",
    "        return pos_loss + 0.5 * aux_loss + 0.1 * reg_loss + 0.1 * contrastive_loss\n",
    "\n",
    "def evaluate_model(model, data_loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in data_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            with autocast():\n",
    "                output = model(data)\n",
    "            pred = output.argmax(dim=1)\n",
    "            correct += pred.eq(target).sum().item()\n",
    "            total += target.size(0)\n",
    "    return 100. * correct / total\n",
    "\n",
    "class EnhancedDataProcessor:\n",
    "    def __init__(self, patch_size=11):\n",
    "        self.patch_size = patch_size\n",
    "        self.scaler = StandardScaler()\n",
    "        \n",
    "    def download_dataset(self):\n",
    "        base_url = \"https://www.ehu.eus/ccwintco/uploads\"\n",
    "        files = {\n",
    "            'Indian_pines.mat': f\"{base_url}/2/22/Indian_pines.mat\",\n",
    "            'Indian_pines_gt.mat': f\"{base_url}/c/c4/Indian_pines_gt.mat\"\n",
    "        }\n",
    "        for filename, url in files.items():\n",
    "            if not os.path.exists(filename):\n",
    "                response = requests.get(url, allow_redirects=True)\n",
    "                with open(filename, 'wb') as f:\n",
    "                    f.write(response.content)\n",
    "\n",
    "    def load_indian_pines(self):\n",
    "        self.download_dataset()\n",
    "        data = sio.loadmat('Indian_pines.mat')['indian_pines']\n",
    "        labels = sio.loadmat('Indian_pines_gt.mat')['indian_pines_gt']\n",
    "        return data, labels\n",
    "\n",
    "    def create_patches(self, data, labels):\n",
    "        padding = self.patch_size // 2\n",
    "        padded_data = np.pad(data, ((padding, padding), (padding, padding), (0, 0)), mode='reflect')\n",
    "        patches = []\n",
    "        patch_labels = []\n",
    "        for i in range(padding, padded_data.shape[0] - padding):\n",
    "            for j in range(padding, padded_data.shape[1] - padding):\n",
    "                if labels[i-padding, j-padding] != 0:\n",
    "                    patch = padded_data[i-padding:i+padding+1, j-padding:j+padding+1, :]\n",
    "                    patches.append(patch)\n",
    "                    patch_labels.append(labels[i-padding, j-padding] - 1)\n",
    "        patches = np.array(patches)\n",
    "        patch_labels = np.array(patch_labels)\n",
    "        patches = np.transpose(patches, (0, 3, 1, 2))\n",
    "        patches = self.scaler.fit_transform(patches.reshape(-1, patches.shape[1])).reshape(patches.shape)\n",
    "        return patches.astype(np.float32), patch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "235317d5-fa26-4b2d-9738-5c958aead59d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_22153/1379377911.py:54: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load('ip_best_model.pth', map_location=device)\n",
      "/tmp/ipykernel_22153/1379377911.py:16: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation Results:\n",
      "--------------------------------------------------\n",
      "Overall Accuracy: 98.62%\n",
      "Average Accuracy: 98.27%\n",
      "Kappa Coefficient: 0.9843\n",
      "\n",
      "Per-Class Accuracy:\n",
      "Class 1: 92.68%\n",
      "Class 2: 97.74%\n",
      "Class 3: 98.80%\n",
      "Class 4: 93.90%\n",
      "Class 5: 97.93%\n",
      "Class 6: 100.00%\n",
      "Class 7: 100.00%\n",
      "Class 8: 100.00%\n",
      "Class 9: 100.00%\n",
      "Class 10: 99.66%\n",
      "Class 11: 98.05%\n",
      "Class 12: 97.19%\n",
      "Class 13: 100.00%\n",
      "Class 14: 100.00%\n",
      "Class 15: 100.00%\n",
      "Class 16: 96.43%\n",
      "\n",
      "Confusion Matrix:\n",
      "[[  38    1    0    0    0    0    0    0    0    2    0    0    0    0\n",
      "     0    0]\n",
      " [   0 1256    0    0    0    7    0    0    0    9   13    0    0    0\n",
      "     0    0]\n",
      " [   0    9  738    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0]\n",
      " [   0    0    6  200    0    0    0    0    0    0    0    7    0    0\n",
      "     0    0]\n",
      " [   0    0    3    0  426    0    1    0    0    0    0    0    5    0\n",
      "     0    0]\n",
      " [   0    0    0    0    0  657    0    0    0    0    0    0    0    0\n",
      "     0    0]\n",
      " [   0    0    0    0    0    0   25    0    0    0    0    0    0    0\n",
      "     0    0]\n",
      " [   0    0    0    0    0    0    0  430    0    0    0    0    0    0\n",
      "     0    0]\n",
      " [   0    0    0    0    0    0    0    0   18    0    0    0    0    0\n",
      "     0    0]\n",
      " [   0    0    0    0    1    0    0    0    0  872    0    0    0    0\n",
      "     2    0]\n",
      " [   0   28    0    0    0    1    0    0    0   14 2167    0    0    0\n",
      "     0    0]\n",
      " [   0    0    0    0    0    0    0    0    0   12    0  519    0    0\n",
      "     2    1]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0  185    0\n",
      "     0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0 1139\n",
      "     0    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "   347    0]\n",
      " [   0    0    0    0    0    0    0    0    0    0    0    3    0    0\n",
      "     0   81]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, cohen_kappa_score\n",
    "from torch.cuda.amp import autocast\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "def evaluate_model_comprehensive(model, test_loader, device, num_classes):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            with autocast():\n",
    "                output = model(data)\n",
    "            pred = output.argmax(dim=1)\n",
    "            all_preds.extend(pred.cpu().numpy())\n",
    "            all_targets.extend(target.cpu().numpy())\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_targets = np.array(all_targets)\n",
    "    \n",
    "    # Calculate confusion matrix\n",
    "    conf_matrix = confusion_matrix(all_targets, all_preds)\n",
    "    \n",
    "    # Calculate Overall Accuracy (OA)\n",
    "    overall_accuracy = np.sum(np.diag(conf_matrix)) / np.sum(conf_matrix)\n",
    "    \n",
    "    # Calculate Per-Class Accuracy\n",
    "    per_class_accuracy = np.diag(conf_matrix) / np.sum(conf_matrix, axis=1)\n",
    "    \n",
    "    # Calculate Average Accuracy (AA)\n",
    "    average_accuracy = np.mean(per_class_accuracy)\n",
    "    \n",
    "    # Calculate Kappa Coefficient\n",
    "    kappa = cohen_kappa_score(all_targets, all_preds)\n",
    "    \n",
    "    return {\n",
    "        'overall_accuracy': overall_accuracy * 100,\n",
    "        'average_accuracy': average_accuracy * 100,\n",
    "        'kappa': kappa,\n",
    "        'per_class_accuracy': per_class_accuracy * 100,\n",
    "        'confusion_matrix': conf_matrix\n",
    "    }\n",
    "\n",
    "def load_and_evaluate():\n",
    "    # Set device\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    # Load the saved model\n",
    "    checkpoint = torch.load('ip_best_model.pth', map_location=device)\n",
    "    \n",
    "    # Initialize data processor and load data\n",
    "    processor = EnhancedDataProcessor()\n",
    "    data, labels = processor.load_indian_pines()\n",
    "    patches, patch_labels = processor.create_patches(data, labels)\n",
    "    \n",
    "    # Create test dataset\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        patches, patch_labels, test_size=0.9, random_state=42, stratify=patch_labels\n",
    "    )\n",
    "    \n",
    "    test_dataset = TensorDataset(torch.FloatTensor(X_test), torch.LongTensor(y_test))\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, \n",
    "        batch_size=16, \n",
    "        num_workers=4, \n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    # Initialize model\n",
    "    num_classes = len(np.unique(patch_labels))\n",
    "    model = RobustHyperspectralEBM(\n",
    "        input_channels=patches.shape[1],\n",
    "        num_classes=num_classes,\n",
    "        device=device\n",
    "    ).to(device)\n",
    "    \n",
    "    # Load the saved state dict\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    # Evaluate the model\n",
    "    metrics = evaluate_model_comprehensive(model, test_loader, device, num_classes)\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\nEvaluation Results:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"Overall Accuracy: {metrics['overall_accuracy']:.2f}%\")\n",
    "    print(f\"Average Accuracy: {metrics['average_accuracy']:.2f}%\")\n",
    "    print(f\"Kappa Coefficient: {metrics['kappa']:.4f}\")\n",
    "    print(\"\\nPer-Class Accuracy:\")\n",
    "    for i, acc in enumerate(metrics['per_class_accuracy']):\n",
    "        print(f\"Class {i+1}: {acc:.2f}%\")\n",
    "    \n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(metrics['confusion_matrix'])\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    metrics = load_and_evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb687722-31c9-4c0b-9f9d-88c08b15b740",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
