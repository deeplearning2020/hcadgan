{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d268c6ce-9265-45fd-ba7d-e0d62e0440f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.utils.data\n",
    "from torch.autograd import Variable\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.decomposition import PCA\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Configuration\n",
    "CONFIG = {\n",
    "    'workers': 0,\n",
    "    'batch_size': 200,\n",
    "    'model_path': 'model/best_model.pth',  # path to trained model\n",
    "    'data_path': 'data',                   # path to dataset directory\n",
    "    'cuda': torch.cuda.is_available(),\n",
    "    'pca_components': 3\n",
    "}\n",
    "\n",
    "if CONFIG['cuda']:\n",
    "    torch.cuda.set_device(0)\n",
    "    cudnn.benchmark = False\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, ndf, nc, nb_label):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.LeakyReLU = nn.LeakyReLU(0.2, inplace=True)\n",
    "        self.conv1 = nn.Conv2d(nc, ndf, 3, 1, 0, bias=False)\n",
    "        self.BatchNorm1 = nn.BatchNorm2d(ndf)\n",
    "        self.conv2 = nn.Conv2d(ndf, ndf * 2, 3, 1, 0, bias=False)\n",
    "        self.BatchNorm2 = nn.BatchNorm2d(ndf * 2)\n",
    "        self.conv3 = nn.Conv2d(ndf * 2, ndf * 4, 3, 1, 0, bias=False)\n",
    "        self.BatchNorm3 = nn.BatchNorm2d(ndf * 4)\n",
    "        self.conv4 = nn.Conv2d(ndf * 4, ndf * 8, 3, 1, 0, bias=False)\n",
    "        self.BatchNorm4 = nn.BatchNorm2d(ndf * 8)\n",
    "        self.conv5 = nn.Conv2d(ndf * 8, ndf * 2, 3, 1, 0, bias=False)\n",
    "        self.aux_linear = nn.Linear(ndf * 2, nb_label + 1)\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "        self.ndf = ndf\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.conv1(input)\n",
    "        x = self.LeakyReLU(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.BatchNorm2(x)\n",
    "        x = self.LeakyReLU(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.BatchNorm3(x)\n",
    "        x = self.LeakyReLU(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.BatchNorm4(x)\n",
    "        x = self.LeakyReLU(x)\n",
    "        x = self.conv5(x)\n",
    "        x = x.view(-1, self.ndf * 2)\n",
    "        c = self.aux_linear(x)\n",
    "        c = self.softmax(c)\n",
    "        return c\n",
    "\n",
    "def applyPCA(X, numComponents):\n",
    "    newX = np.reshape(X, (-1, X.shape[2]))\n",
    "    pca = PCA(n_components=numComponents, whiten=True)\n",
    "    newX = pca.fit_transform(newX)\n",
    "    newX = np.reshape(newX, (X.shape[0], X.shape[1], numComponents))\n",
    "    return newX\n",
    "\n",
    "def padWithZeros(X, margin=5):\n",
    "    newX = np.zeros((X.shape[0] + 2 * margin, X.shape[1] + 2 * margin, X.shape[2]))\n",
    "    newX[margin:X.shape[0] + margin, margin:X.shape[1] + margin, :] = X\n",
    "    return newX\n",
    "\n",
    "def createImageCubes(X, y, windowSize=11, removeZeroLabels=True):\n",
    "    margin = int((windowSize - 1) / 2)\n",
    "    zeroPaddedX = padWithZeros(X, margin=margin)\n",
    "    patchesData = []\n",
    "    patchesLabels = []\n",
    "    for r in range(margin, zeroPaddedX.shape[0] - margin):\n",
    "        for c in range(margin, zeroPaddedX.shape[1] - margin):\n",
    "            patch = zeroPaddedX[r - margin:r + margin + 1, c - margin:c + margin + 1]\n",
    "            label = y[r-margin, c-margin]\n",
    "            if label > 0 or not removeZeroLabels:\n",
    "                patchesData.append(patch)\n",
    "                patchesLabels.append(label)\n",
    "    return np.array(patchesData), np.array(patchesLabels)\n",
    "\n",
    "class TestDS(torch.utils.data.Dataset):\n",
    "    def __init__(self, x_data, y_data):\n",
    "        self.len = x_data.shape[0]\n",
    "        self.x_data = torch.FloatTensor(x_data)\n",
    "        self.y_data = torch.LongTensor(y_data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.x_data[index], self.y_data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "def load_data(data_path):\n",
    "    matfn1 = os.path.join(data_path, 'Indian_pines_corrected.mat')\n",
    "    data1 = sio.loadmat(matfn1)\n",
    "    X = data1['indian_pines_corrected']\n",
    "    matfn2 = os.path.join(data_path, 'Indian_pines_gt.mat')\n",
    "    data2 = sio.loadmat(matfn2)\n",
    "    y = data2['indian_pines_gt']\n",
    "    return X, y\n",
    "\n",
    "def prepare_data(X, y, pca_components=3):\n",
    "    X_pca = applyPCA(X, numComponents=pca_components)\n",
    "    patches, labels = createImageCubes(X_pca, y, windowSize=11)\n",
    "    labels = labels - 1\n",
    "    return patches.transpose(0, 3, 1, 2).astype('float32'), labels\n",
    "\n",
    "def kappa(confusion_matrix):\n",
    "    n = np.sum(confusion_matrix)\n",
    "    sum_po = np.sum(np.diag(confusion_matrix))\n",
    "    po = sum_po / n\n",
    "    pe = np.sum(np.sum(confusion_matrix, axis=0) * np.sum(confusion_matrix, axis=1)) / (n * n)\n",
    "    return (po - pe) / (1 - pe)\n",
    "\n",
    "def evaluate_model(model, test_loader, criterion, num_classes, device):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in tqdm(test_loader, desc='Evaluating'):\n",
    "            if device == 'cuda':\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            \n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item()\n",
    "            pred = output.max(1)[1]\n",
    "            all_preds.extend(pred.cpu().numpy())\n",
    "            all_targets.extend(target.cpu().numpy())\n",
    "            correct += pred.eq(target).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    \n",
    "    # Calculate confusion matrix and metrics\n",
    "    conf_matrix = confusion_matrix(np.array(all_targets), np.array(all_preds))\n",
    "    class_acc = np.diag(conf_matrix) / np.sum(conf_matrix, axis=1)\n",
    "    average_accuracy = np.mean(class_acc)\n",
    "    kappa_score = kappa(conf_matrix)\n",
    "    \n",
    "    return {\n",
    "        'test_loss': test_loss,\n",
    "        'accuracy': accuracy,\n",
    "        'average_accuracy': average_accuracy * 100,\n",
    "        'kappa': kappa_score,\n",
    "        'confusion_matrix': conf_matrix,\n",
    "        'class_accuracies': class_acc * 100\n",
    "    }\n",
    "\n",
    "def main():\n",
    "    # Load and prepare data\n",
    "    X, y = load_data(CONFIG['data_path'])\n",
    "    X_test, y_test = prepare_data(X, y, CONFIG['pca_components'])\n",
    "    \n",
    "    # Create test dataset and loader\n",
    "    testset = TestDS(X_test, y_test)\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        testset, \n",
    "        batch_size=CONFIG['batch_size'],\n",
    "        shuffle=False,\n",
    "        num_workers=CONFIG['workers']\n",
    "    )\n",
    "    \n",
    "    # Load model\n",
    "    checkpoint = torch.load(CONFIG['model_path'])\n",
    "    num_classes = len(np.unique(y_test))\n",
    "    model = Discriminator(ndf=64, nc=CONFIG['pca_components'], nb_label=num_classes)\n",
    "    model.load_state_dict(checkpoint['state_dict_D'])\n",
    "    \n",
    "    device = 'cuda' if CONFIG['cuda'] else 'cpu'\n",
    "    if CONFIG['cuda']:\n",
    "        model = model.cuda()\n",
    "    \n",
    "    criterion = nn.NLLLoss()\n",
    "    if CONFIG['cuda']:\n",
    "        criterion = criterion.cuda()\n",
    "    \n",
    "    # Evaluate model\n",
    "    results = evaluate_model(model, test_loader, criterion, num_classes, device)\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\nEvaluation Results:\")\n",
    "    print(f\"Test Loss: {results['test_loss']:.4f}\")\n",
    "    print(f\"Overall Accuracy: {results['accuracy']:.2f}%\")\n",
    "    print(f\"Average Accuracy: {results['average_accuracy']:.2f}%\")\n",
    "    print(f\"Kappa Coefficient: {results['kappa']:.4f}\")\n",
    "    \n",
    "    print(\"\\nClass-wise Accuracies:\")\n",
    "    for i, acc in enumerate(results['class_accuracies']):\n",
    "        print(f\"Class {i+1}: {acc:.2f}%\")\n",
    "    \n",
    "    # Save confusion matrix\n",
    "    np.save('confusion_matrix.npy', results['confusion_matrix'])\n",
    "    print(\"\\nConfusion matrix saved as 'confusion_matrix.npy'\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
